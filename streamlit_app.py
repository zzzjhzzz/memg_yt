import re
import random
from time import sleep
import html
from typing import Optional, List
from urllib.parse import urlparse, parse_qs
from urllib.request import urlopen
import ssl

import streamlit as st
from youtube_transcript_api import (
YouTubeTranscriptApi,
NoTranscriptFound,
TranscriptsDisabled,
VideoUnavailable,
)
from pytube import YouTube
import yt_dlp

# ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤ ì •ì˜
class TranscriptExtractionError(Exception):
"""ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ì»¤ìŠ¤í…€ ì˜ˆì™¸"""
pass
from pytube import YouTube
import yt_dlp

# SSL ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°
ssl._create_default_https_context = ssl._create_unverified_context

# ---------------------------------
# URL ì •ë¦¬ / ë¹„ë””ì˜¤ID ì¶”ì¶œ
# ---------------------------------
YOUTUBE_URL_RE = re.compile(
r'(?:https?://)?(?:www\.)?(?:youtube\.com/(?:watch\?v=|embed/|live/|shorts/)|youtu\.be/)([\w-]{11})(?:\S+)?'
)

def extract_video_id(url: str) -> Optional[str]:
if not url:
return None

# ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë¨¼ì € ì‹œë„
m = YOUTUBE_URL_RE.search(url)
if m:
return m.group(1)

# URL íŒŒì‹±ìœ¼ë¡œ ì¬ì‹œë„
try:
parsed = urlparse(url)
if parsed.hostname in ['youtube.com', 'www.youtube.com']:
vid = parse_qs(parsed.query).get("v", [None])[0]
if vid and len(vid) == 11:
return vid
elif parsed.hostname in ['youtu.be', 'www.youtu.be']:
vid = parsed.path.lstrip('/')
if len(vid) == 11:
return vid
except Exception:
pass

return None

def to_clean_watch_url(url_or_id: str) -> str:
"""ì§§ì€ ì£¼ì†Œ/íŒŒë¼ë¯¸í„°ë¥¼ í‘œì¤€ watch URLë¡œ ì •ë¦¬."""
vid = extract_video_id(url_or_id) if "http" in url_or_id else url_or_id
return f"https://www.youtube.com/watch?v={vid}" if vid else url_or_id

def safe_get_youtube_info(url: str):
"""yt-dlpë¡œ ì•ˆì „í•œ YouTube ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
try:
ydl_opts = {
"quiet": True,
"noplaylist": True,
"extract_flat": False,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
info = ydl.extract_info(url, download=False)

# ê°„ë‹¨í•œ ì •ë³´ ê°ì²´ ìƒì„±
class YouTubeInfo:
def __init__(self, info_dict):
self.title = info_dict.get('title', 'ì œëª© í™•ì¸ ë¶ˆê°€')
self.length = info_dict.get('duration', 0)

return YouTubeInfo(info)

except Exception as e:
st.warning(f"YouTube ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
return None

# ---------------------------------
# 1) youtube_transcript_api (ê³µì‹/ìë™ìƒì„±)
# ---------------------------------
def fetch_via_yta_with_retry(video_id: str, langs: List[str], max_retries: int = 3) -> str:
"""ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ YTA ìë§‰ ì¶”ì¶œ (ì¡°ìš©í•œ ë²„ì „)"""
last_error = None

for attempt in range(max_retries):
try:
tl = YouTubeTranscriptApi.list_transcripts(video_id)

# ì—…ë¡œë” ìë§‰ ë¨¼ì € ì‹œë„
try:
tr = tl.find_transcript(langs)
except Exception:
# ìë™ìƒì„± ìë§‰ìœ¼ë¡œ í´ë°±
tr = tl.find_generated_transcript(langs)

entries = tr.fetch()
# ì„±ê³µ ì‹œì—ë§Œ ë©”ì‹œì§€ í‘œì‹œ
st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (YTA): {tr.language}" + (" [ìë™ìƒì„±]" if tr.is_generated else " [ìˆ˜ë™]"))
return "\n".join([f"[{e['start']:.1f}] {e['text']}" for e in entries])

except Exception as e:
last_error = e
error_msg = str(e).lower()

if "too many requests" in error_msg or "429" in error_msg:
if attempt < max_retries - 1:
wait_time = (2 ** attempt) + random.uniform(1, 3)
sleep(wait_time)
continue
else:
raise TranscriptExtractionError(f"YouTube API ìš”ì²­ ì œí•œ ì´ˆê³¼ (429)")
else:
# ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ëŠ” ì¦‰ì‹œ ì¬ë°œìƒ
if isinstance(e, (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable)):
raise
else:
raise TranscriptExtractionError(f"YTA ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

raise TranscriptExtractionError(f"YTA ì¬ì‹œë„ ì‹¤íŒ¨: {str(last_error)}")

# ---------------------------------
# 2) pytube captions í´ë°± (SRT/XML)
# ---------------------------------
def clean_xml_text(xml_text: str) -> List[tuple]:
"""XMLì—ì„œ (start, text) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜."""
items = []
xml_text = xml_text.replace("\n", "")
pattern = r'<text[^>]*start="([\d\.]+)"[^>]*>(.*?)</text>'

for m in re.finditer(pattern, xml_text, re.DOTALL):
try:
start = float(m.group(1))
raw = re.sub(r"<.*?>", " ", m.group(2))
text = html.unescape(raw)
text = re.sub(r"\s+", " ", text).strip()
if text:
items.append((start, text))
except ValueError:
continue
return items

def fetch_via_pytube(url_or_id: str, langs: List[str]) -> str:
"""pytube ìë§‰ íŠ¸ë™ì—ì„œ ì¶”ì¶œ (ì¡°ìš©í•œ ë²„ì „)."""
url = to_clean_watch_url(url_or_id)

try:
# ë” ê´€ëŒ€í•œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
yt = YouTube(url, use_oauth=False, allow_oauth_cache=False)

# ì •ë³´ ë¡œë”©ì„ ëª…ì‹œì ìœ¼ë¡œ ì‹œë„
try:
_ = yt.title  # ê¸°ë³¸ ì •ë³´ ë¡œë”© í…ŒìŠ¤íŠ¸
except Exception:
# User-Agent ë³€ê²½í•´ì„œ ì¬ì‹œë„
import urllib.request
opener = urllib.request.build_opener()
opener.addheaders = [('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')]
urllib.request.install_opener(opener)

yt = YouTube(url, use_oauth=False, allow_oauth_cache=False)
_ = yt.title

tracks = yt.captions
if not tracks:
raise TranscriptExtractionError("pytube: ìë§‰ íŠ¸ë™ì´ ì—†ìŒ")

# ì„ í˜¸ ì–¸ì–´ ì½”ë“œ + ìë™ìƒì„± ì½”ë“œ í›„ë³´ êµ¬ì„±
candidates = []
for lg in langs:
candidates.append(lg)
candidates.append(f"a.{lg}")

# ì˜ì–´ í´ë°±
if "en" not in [c.replace("a.", "") for c in candidates]:
candidates.extend(["en", "a.en"])

available_codes = {c.code: c for c in tracks}

for code in candidates:
cap = available_codes.get(code)

# ì§€ì—­ì½”ë“œ ë§¤ì¹­ ì‹œë„ (ì˜ˆ: ko-KR)
if not cap:
for k, v in available_codes.items():
if k.lower().startswith(code.lower().replace("a.", "")):
cap = v
code = k  # ì‹¤ì œ ë°œê²¬ëœ ì½”ë“œë¡œ ì—…ë°ì´íŠ¸
break

if not cap:
continue

try:
# SRT í˜•ì‹ìœ¼ë¡œ ì‹œë„
srt = cap.generate_srt_captions()
lines = []

for block in srt.strip().split("\n\n"):
if not block.strip():
continue

parts = block.split("\n")
if len(parts) >= 3:
# íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±
ts = parts[1].split("-->")[0].strip()
try:
h, m, s_ms = ts.split(":")
s, ms = s_ms.split(",")
start = int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0
text = " ".join(parts[2:]).strip()
if text:
lines.append(f"[{start:.1f}] {text}")
except (ValueError, IndexError):
continue

if lines:
st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (pytube): {code}")
return "\n".join(lines)

except Exception:
# XML í˜•ì‹ìœ¼ë¡œ í´ë°±
try:
xml = cap.xml_captions
items = clean_xml_text(xml)
if items:
st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (pytube): {code}")
return "\n".join([f"[{stt:.1f}] {txt}" for stt, txt in items])
except Exception:
continue

except Exception as e:
raise TranscriptExtractionError(f"pytube ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    raise TranscriptExtractionError(f"pytube: ë§¤ì¹­ë˜ëŠ” ìë§‰ ì—†ìŒ (ì‚¬ìš©ê°€ëŠ¥: {list(available_codes.keys())})")
    raise TranscriptExtractionError(f"pytube: ë§¤ì¹­ë˜ëŠ” ìë§‰ ì—†ìŒ (ì‚¬ìš©ê°€ëŠ¥: {list(available_codes.keys()) if 'available_codes' in locals() else 'N/A'})")

# ---------------------------------
# 3) yt-dlp í´ë°±
# ---------------------------------
def parse_vtt(vtt: str) -> List[str]:
"""WebVTTë¥¼ [start] text í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
lines = []
blocks = [b for b in vtt.strip().split("\n\n") if "-->" in b]

for block in blocks:
rows = block.split("\n")
if not rows:
continue

ts = rows[0]
m = re.match(r"(\d+):(\d+):(\d+(?:\.\d+)?)", ts.replace(",", "."))

start = 0.0
if m:
h, m_, s = m.groups()
start = int(h) * 3600 + int(m_) * 60 + float(s)

text = " ".join(rows[1:]).strip()
text = re.sub(r"<.*?>", " ", text)
text = re.sub(r"\s+", " ", text)
if text:
lines.append(f"[{start:.1f}] {text}")

return lines

def parse_srv3_json(json_data: str) -> List[str]:
    """YouTube SRV3 JSON ìë§‰ íŒŒì‹±"""
    try:
        import json
        data = json.loads(json_data)
        lines = []
        
        events = data.get("events", [])
        for event in events:
            start_time = event.get("tStartMs", 0) / 1000.0
            segs = event.get("segs", [])
            text = "".join([seg.get("utf8", "") for seg in segs]).strip()
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        
        return lines
    except Exception:
        return []

def parse_ttml(ttml_data: str) -> List[str]:
    """TTML XML ìë§‰ íŒŒì‹±"""
    try:
        lines = []
        # <p> íƒœê·¸ì—ì„œ ì‹œê°„ ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pattern = r'<p[^>]*begin="([^"]*)"[^>]*>(.*?)</p>'
        
        for match in re.finditer(pattern, ttml_data, re.DOTALL):
            time_str = match.group(1)
            text_content = match.group(2)
            
            # ì‹œê°„ ë³€í™˜ (00:00:12.340 -> ì´ˆ)
            try:
                parts = time_str.replace(',', '.').split(':')
                if len(parts) == 3:
                    h, m, s = parts
                    start_time = int(h) * 3600 + int(m) * 60 + float(s)
                else:
                    start_time = 0.0
            except:
                start_time = 0.0
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            text = re.sub(r"<.*?>", " ", text_content)
            text = html.unescape(text)
            text = re.sub(r"\s+", " ", text).strip()
            
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        
        return lines
    except Exception:
        return []

def fetch_via_ytdlp_enhanced(url_or_id: str, langs: List[str]) -> str:
"""í–¥ìƒëœ yt-dlp ìë§‰ ê°€ì ¸ì˜¤ê¸° (ì¡°ìš©í•œ ë²„ì „)"""
url = to_clean_watch_url(url_or_id)

# ë” ê´€ëŒ€í•œ ì„¤ì •
ydl_opts = {
"quiet": True,
"no_warnings": True,
"noplaylist": True,
"writesubtitles": False,
"writeautomaticsub": False,
"socket_timeout": 60,
"retries": 3,
# User-Agent ìˆœí™˜ ì‚¬ìš©
"http_headers": {
"User-Agent": random.choice([
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
])
}
}

try:
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
info = ydl.extract_info(url, download=False)
except Exception as e:
raise TranscriptExtractionError(f"yt-dlp ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

subs = info.get("subtitles") or {}
autos = info.get("automatic_captions") or {}

# í›„ë³´ êµ¬ì„± (ë” ë„“ì€ ë²”ìœ„)
candidates = []

# 1ìˆœìœ„: ìš”ì²­í•œ ì–¸ì–´ì˜ ìˆ˜ë™ ìë§‰
for lg in langs:
if lg in subs:
candidates.append(("manual", lg, subs[lg]))

# 2ìˆœìœ„: ìš”ì²­í•œ ì–¸ì–´ì˜ ìë™ ìë§‰
for lg in langs:
if lg in autos:
candidates.append(("auto", lg, autos[lg]))

# 3ìˆœìœ„: ì˜ì–´ í´ë°±
if "en" not in langs:
if "en" in subs:
candidates.append(("manual", "en", subs["en"]))
if "en" in autos:
candidates.append(("auto", "en", autos["en"]))

# 4ìˆœìœ„: ë‹¤ë¥¸ ì–¸ì–´ë¼ë„ ì‹œë„ (ì²« ë²ˆì§¸ ê°€ëŠ¥í•œ ê²ƒ)
if not candidates:
all_available = list(subs.keys()) + list(autos.keys())
if all_available:
first_lang = all_available[0]
if first_lang in subs:
candidates.append(("manual", first_lang, subs[first_lang]))
elif first_lang in autos:
candidates.append(("auto", first_lang, autos[first_lang]))

# í˜•ì‹ ìš°ì„ ìˆœìœ„ ì •ì˜
format_priority = ["vtt", "webvtt", "srv3", "ttml", "json3"]

for kind, lg, fmt_list in candidates:
if not fmt_list:
continue

# í˜•ì‹ì„ ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì •ë ¬
sorted_formats = []
for fmt_name in format_priority:
for item in fmt_list:
if item.get("ext", "").lower() == fmt_name:
sorted_formats.append(item)

# ë‚˜ë¨¸ì§€ í˜•ì‹ ì¶”ê°€
for item in fmt_list:
if item not in sorted_formats:
sorted_formats.append(item)

for item in sorted_formats:
try:
with urlopen(item["url"]) as resp:
data = resp.read().decode("utf-8", errors="ignore")

ext = item.get("ext", "").lower()

if ext in ("vtt", "webvtt"):
lines = parse_vtt(data)
if lines:
st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, {ext.upper()})")
return "\n".join(lines)

elif ext == "srv3":
lines = parse_srv3_json(data)
if lines:
st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, SRV3)")
return "\n".join(lines)

elif ext == "ttml":
lines = parse_ttml(data)
if lines:
st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, TTML)")
return "\n".join(lines)

else:
# ë‹¤ë¥¸ í˜•ì‹: ë‹¨ìˆœ íƒœê·¸ ì œê±°
text = re.sub(r"<.*?>", " ", data)
text = html.unescape(text)
text = re.sub(r"\s+", " ", text).strip()
if text and len(text) > 100:
st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, {ext.upper()})")
return text

except Exception:
continue  # ì¡°ìš©íˆ ë‹¤ìŒ í˜•ì‹ ì‹œë„

# ë””ë²„ê¹… ì •ë³´ (ì‹¤íŒ¨í–ˆì„ ë•Œë§Œ)
available_langs = list(set(list(subs.keys()) + list(autos.keys())))
raise TranscriptExtractionError(f"yt-dlp: ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ (ì‚¬ìš©ê°€ëŠ¥: {available_langs})")

def parse_srv3_json(json_data: str) -> List[str]:
    """YouTube SRV3 JSON ìë§‰ íŒŒì‹±"""
    try:
        import json
        data = json.loads(json_data)
        lines = []
        
        events = data.get("events", [])
        for event in events:
            start_time = event.get("tStartMs", 0) / 1000.0
            segs = event.get("segs", [])
            text = "".join([seg.get("utf8", "") for seg in segs]).strip()
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        
        return lines
    except Exception:
        return []

def parse_ttml(ttml_data: str) -> List[str]:
    """TTML XML ìë§‰ íŒŒì‹±"""
    try:
        lines = []
        # <p> íƒœê·¸ì—ì„œ ì‹œê°„ ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pattern = r'<p[^>]*begin="([^"]*)"[^>]*>(.*?)</p>'
        
        for match in re.finditer(pattern, ttml_data, re.DOTALL):
            time_str = match.group(1)
            text_content = match.group(2)
            
            # ì‹œê°„ ë³€í™˜ (00:00:12.340 -> ì´ˆ)
            try:
                parts = time_str.replace(',', '.').split(':')
                if len(parts) == 3:
                    h, m, s = parts
                    start_time = int(h) * 3600 + int(m) * 60 + float(s)
                else:
                    start_time = 0.0
            except:
                start_time = 0.0
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            text = re.sub(r"<.*?>", " ", text_content)
            text = html.unescape(text)
            text = re.sub(r"\s+", " ", text).strip()
            
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        
        return lines
    except Exception:
        return []

# ---------------------------------
# ìµœì¢… ë˜í¼
# ---------------------------------
def fetch_transcript_resilient(url: str, video_id: str, langs: List[str]) -> str:
"""3ë‹¨ê³„ í´ë°±ìœ¼ë¡œ ìë§‰ ê°€ì ¸ì˜¤ê¸° (ê¹”ë”í•œ ë²„ì „)"""
errors = []

# 1) youtube_transcript_api (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
try:
return fetch_via_yta_with_retry(video_id, langs)
except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable) as e:
errors.append(f"YTA: {str(e)}")
sleep(1)
except TranscriptExtractionError as e:
errors.append(f"YTA: {str(e)}")
sleep(1)
except Exception as e:
errors.append(f"YTA: {str(e)}")
sleep(1)

# 2) yt-dlp (í–¥ìƒëœ ë²„ì „)
try:
return fetch_via_ytdlp_enhanced(url, langs)
except TranscriptExtractionError as e:
errors.append(f"yt-dlp: {str(e)}")
sleep(1)
except Exception as e:
errors.append(f"yt-dlp: {str(e)}")
sleep(1)

# 3) pytube (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
try:
return fetch_via_pytube(url, langs)
except TranscriptExtractionError as e:
errors.append(f"pytube: {str(e)}")
except Exception as e:
errors.append(f"pytube: {str(e)}")

# ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ - ì˜¤ë¥˜ ì •ë³´ë¥¼ expanderì— ë„£ì–´ì„œ ì ‘ì„ ìˆ˜ ìˆê²Œ í•¨
with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´", expanded=False):
for i, error in enumerate(errors, 1):
st.text(f"{i}. {error}")

# ê°„ë‹¨í•œ ì˜¤ë¥˜ ë©”ì‹œì§€
if any("429" in err or "Too Many Requests" in err for err in errors):
raise TranscriptExtractionError("YouTube API ìš”ì²­ ì œí•œ (429) - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì˜ìƒì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”")
elif any("ìë§‰" in err and ("ì—†ìŒ" in err or "ì°¾ì„ ìˆ˜ ì—†ìŒ" in err) for err in errors):
raise TranscriptExtractionError("ì´ ì˜ìƒì—ëŠ” ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤")
else:
        raise TranscriptExtractionError("ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ - ìœ„ì˜ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”") e:
        errors.append(f"yt-dlp: {str(e)}")
        sleep(1)
    except Exception as e:
        errors.append(f"yt-dlp: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - {str(e)}")
        sleep(1)

    # 3) pytube (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    try:
        return fetch_via_pytube(url, langs)
    except TranscriptExtractionError as e:
        errors.append(f"pytube: {str(e)}")
    except Exception as e:
        errors.append(f"pytube: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - {str(e)}")

    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ìƒì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì œê³µ
    error_msg = " | ".join(errors)
    raise TranscriptExtractionError(f"ëª¨ë“  ë°©ë²• ì‹¤íŒ¨: {error_msg}")
        raise TranscriptExtractionError("ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ - ìœ„ì˜ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”")

# ---------------------------------
# Streamlit UI
# ---------------------------------
st.set_page_config(page_title="YouTube ìë§‰ ì¶”ì¶œê¸°", layout="wide")
st.title("ğŸ¬ YouTube ìë§‰ ì¶”ì¶œê¸°")
st.caption("YouTube ì˜ìƒì˜ ìë§‰ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. API í‚¤ ë¶ˆí•„ìš”.")

with st.sidebar:
st.header("ì„¤ì •")
lang_pref = st.multiselect(
"ì–¸ì–´ ìš°ì„ ìˆœìœ„ (ìœ„ì—ì„œë¶€í„° ì‹œë„)",
["ko", "en", "ja", "zh-Hans", "zh-Hant", "es", "fr", "de"],
default=["ko", "en"],
help="ì„ í˜¸í•˜ëŠ” ì–¸ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì„ íƒí•˜ì„¸ìš”"
)
show_meta = st.toggle("ì˜ìƒ ì œëª©/ê¸¸ì´ í‘œì‹œ", value=True)

url = st.text_input(
"YouTube ë§í¬", 
placeholder="https://www.youtube.com/watch?v=... ë˜ëŠ” https://youtu.be/...",
help="YouTube ì˜ìƒì˜ URLì„ ì…ë ¥í•˜ì„¸ìš”"
)

run = st.button("ìë§‰ ì¶”ì¶œ", type="primary")

if run:
if not url.strip():
st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
st.stop()

# URL ì •ë¦¬ ë° ë¹„ë””ì˜¤ ID ì¶”ì¶œ
clean_url = to_clean_watch_url(url.strip())
vid = extract_video_id(clean_url)

if not vid:
st.error("ìœ íš¨í•œ YouTube ë§í¬ê°€ ì•„ë‹™ë‹ˆë‹¤. URLì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
st.stop()

st.info(f"ë¹„ë””ì˜¤ ID: {vid}")

# ë©”íƒ€ ì •ë³´ í‘œì‹œ
if show_meta:
with st.spinner("ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
try:
info = safe_get_youtube_info(clean_url)
if info:
title = info.title
length_min = int((info.length or 0) / 60) if info.length else 0
st.info(f"**ì œëª©**: {title}  |  **ê¸¸ì´**: ì•½ {length_min}ë¶„")
else:
st.caption("ì˜ìƒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ - ìë§‰ ì¶”ì¶œì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
except Exception as e:
st.caption(f"ì˜ìƒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({str(e)[:50]}) - ìë§‰ ì¶”ì¶œì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

# ìë§‰ ì¶”ì¶œ
with st.spinner("ìë§‰ ì¶”ì¶œ ì¤‘..."):
try:
transcript_text = fetch_transcript_resilient(clean_url, vid, lang_pref)
except TranscriptExtractionError as e:
st.error(f"âŒ {str(e)}")
st.stop()
except (NoTranscriptFound, TranscriptsDisabled) as e:
st.error(f"âŒ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
st.stop()
except VideoUnavailable:
st.error("âŒ ì˜ìƒì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë¹„ê³µê°œ, ì§€ì—­ì œí•œ, ì—°ë ¹ì œí•œ ë“±)")
st.stop()
except Exception as e:
st.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
st.stop()

# ê²°ê³¼ ì¶œë ¥
st.success("ìë§‰ ì¶”ì¶œ ì™„ë£Œ!")

col1, col2 = st.columns([1, 4])
with col1:
st.download_button(
"ğŸ“„ ìë§‰ ë‹¤ìš´ë¡œë“œ (TXT)",
data=transcript_text.encode("utf-8"),
file_name=f"transcript_{vid}.txt",
mime="text/plain",
)

with col2:
word_count = len(transcript_text.split())
st.caption(f"ì´ {word_count:,}ê°œ ë‹¨ì–´")

st.subheader("ğŸ“„ ì¶”ì¶œëœ ìë§‰")
st.text_area(
"", 
value=transcript_text, 
height=500,
help="ìë§‰ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ë³µì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
)

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
st.caption(
"ğŸ’¡ **ì‚¬ìš© íŒ**: ì´ ë„êµ¬ëŠ” ê°œì¸ í•™ìŠµ/ì—°êµ¬ ëª©ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. "
"ì¼ë¶€ ì˜ìƒì€ ì €ì‘ê¶Œ, ì—°ë ¹ì œí•œ, ì§€ì—­ì œí•œ ë“±ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)
