import re
import random
from time import sleep
import html
from typing import Optional, List
from urllib.parse import urlparse, parse_qs
import ssl
import json
import urllib.request

import streamlit as st
from youtube_transcript_api import (
    YouTubeTranscriptApi,
    NoTranscriptFound,
    TranscriptsDisabled,
    VideoUnavailable,
)
from pytube import YouTube
import yt_dlp

# ---- ì»¤ìŠ¤í…€ ì˜ˆì™¸ ----
class TranscriptExtractionError(Exception):
    pass

# ---- SSL ì™„í™” (ì¼ë¶€ í˜¸ìŠ¤íŒ… í™˜ê²½ìš©) ----
ssl._create_default_https_context = ssl._create_unverified_context

# ---- ê³µí†µ í—¤ë”/ì¿ í‚¤ (ìë§‰ íŒŒì¼ GETì—ë„ ë™ì¼ ì ìš©) ----
def build_common_headers(ua: str = None) -> dict:
    return {
        "User-Agent": ua
        or random.choice(
            [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            ]
        ),
        # ì–¸ì–´ íŒíŠ¸
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        # ê°„ë‹¨ CONSENT íšŒí”¼ìš© ì¿ í‚¤
        "Cookie": "CONSENT=YES+cb",
        "Accept": "*/*",
        "Connection": "close",
    }

def urlopen_with_headers(url: str, headers: dict, timeout: int = 30, retries: int = 3):
    """
    ìë§‰/JSON/VTT ìš”ì²­ ì‹œ í—¤ë”ë¥¼ ë™ì¼í•˜ê²Œ ì „ë‹¬í•˜ê³  429/403 ë“± ì¼ì‹œì˜¤ë¥˜ì— ì¬ì‹œë„.
    """
    last_err = None
    for attempt in range(retries):
        try:
            opener = urllib.request.build_opener()
            opener.addheaders = list(headers.items())
            with opener.open(url, timeout=timeout) as resp:
                return resp.read()
        except Exception as e:
            last_err = e
            msg = str(e).lower()
            if any(x in msg for x in ["429", "too many requests", "temporarily", "timed out", "403", "unavailable"]):
                sleep((2 ** attempt) + random.uniform(0.5, 1.5))
                continue
            break
    raise last_err

# ---- URL/ID ìœ í‹¸ ----
YOUTUBE_URL_RE = re.compile(
    r'(?:https?://)?(?:www\.)?(?:youtube\.com/(?:watch\?v=|embed/|live/|shorts/)|youtu\.be/)([\w-]{11})(?:\S+)?'
)

def extract_video_id(url: str) -> Optional[str]:
    if not url:
        return None
    m = YOUTUBE_URL_RE.search(url)
    if m:
        return m.group(1)
    try:
        parsed = urlparse(url)
        if parsed.hostname in ["youtube.com", "www.youtube.com"]:
            vid = parse_qs(parsed.query).get("v", [None])[0]
            if vid and len(vid) == 11:
                return vid
        elif parsed.hostname in ["youtu.be", "www.youtu.be"]:
            vid = parsed.path.lstrip("/")
            if len(vid) == 11:
                return vid
    except Exception:
        pass
    return None

def to_clean_watch_url(url_or_id: str) -> str:
    vid = extract_video_id(url_or_id) if "http" in url_or_id else url_or_id
    return f"https://www.youtube.com/watch?v={vid}" if vid else url_or_id

# ---- ë©”íƒ€ ì •ë³´ (ìš”ì²­ ìµœì†Œí™”) ----
def safe_get_youtube_info(url: str):
    try:
        ydl_opts = {"quiet": True, "noplaylist": True, "extract_flat": False}
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
        class YouTubeInfo:
            def __init__(self, d):
                self.title = d.get("title", "ì œëª© í™•ì¸ ë¶ˆê°€")
                self.length = d.get("duration", 0)
        return YouTubeInfo(info)
    except Exception as e:
        st.caption(f"ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)[:60]}")
        return None

# ---- YTA (1ì°¨ ì‹œë„, ë‚´ë¶€ ì¬ì‹œë„) ----
def fetch_via_yta_with_retry(video_id: str, langs: List[str], max_retries: int = 3) -> str:
    last_error = None
    for attempt in range(max_retries):
        try:
            tl = YouTubeTranscriptApi.list_transcripts(video_id)
            try:
                tr = tl.find_transcript(langs)
            except Exception:
                tr = tl.find_generated_transcript(langs)
            entries = tr.fetch()
            st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (YTA): {tr.language}" + (" [ìë™]" if tr.is_generated else " [ìˆ˜ë™]"))
            return "\n".join([f"[{e['start']:.1f}] {e['text']}" for e in entries])
        except Exception as e:
            last_error = e
            msg = str(e).lower()
            if ("429" in msg or "too many requests" in msg) and attempt < max_retries - 1:
                sleep((2 ** attempt) + random.uniform(1, 3))
                continue
            if isinstance(e, (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable)):
                raise
            raise TranscriptExtractionError(f"YTA ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    raise TranscriptExtractionError(f"YTA ì¬ì‹œë„ ì‹¤íŒ¨: {str(last_error)}")

# ---- pytube (3ì°¨ í´ë°±) ----
def clean_xml_text(xml_text: str) -> List[tuple]:
    items = []
    xml_text = xml_text.replace("\n", "")
    pattern = r'<text[^>]*start="([\d\.]+)"[^>]*>(.*?)</text>'
    for m in re.finditer(pattern, xml_text, re.DOTALL):
        try:
            start = float(m.group(1))
            raw = re.sub(r"<.*?>", " ", m.group(2))
            text = html.unescape(raw)
            text = re.sub(r"\s+", " ", text).strip()
            if text:
                items.append((start, text))
        except ValueError:
            continue
    return items

def fetch_via_pytube(url_or_id: str, langs: List[str]) -> str:
    url = to_clean_watch_url(url_or_id)
    try:
        yt = YouTube(url, use_oauth=False, allow_oauth_cache=False)
        _ = yt.title
        tracks = yt.captions
        if not tracks:
            raise TranscriptExtractionError("pytube: ìë§‰ íŠ¸ë™ì´ ì—†ìŒ")

        candidates = []
        for lg in langs:
            candidates += [lg, f"a.{lg}"]
        if "en" not in [c.replace("a.", "") for c in candidates]:
            candidates += ["en", "a.en"]

        available = {c.code: c for c in tracks}
        for code in list(candidates):
            cap = available.get(code)
            if not cap:
                # ko-KR ê°™ì€ ì§€ì—­ì½”ë“œ ë§¤ì¹­
                for k, v in available.items():
                    if k.lower().startswith(code.lower().replace("a.", "")):
                        cap = v
                        code = k
                        break
            if not cap:
                continue
            try:
                srt = cap.generate_srt_captions()
                lines = []
                for block in srt.strip().split("\n\n"):
                    parts = block.split("\n")
                    if len(parts) >= 3:
                        ts = parts[1].split("-->")[0].strip()
                        try:
                            h, m, s_ms = ts.split(":")
                            s, ms = s_ms.split(",")
                            start = int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0
                            text = " ".join(parts[2:]).strip()
                            if text:
                                lines.append(f"[{start:.1f}] {text}")
                        except Exception:
                            continue
                if lines:
                    st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (pytube): {code}")
                    return "\n".join(lines)
            except Exception:
                try:
                    xml = cap.xml_captions
                    items = clean_xml_text(xml)
                    if items:
                        st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (pytube): {code}")
                        return "\n".join([f"[{stt:.1f}] {txt}" for stt, txt in items])
                except Exception:
                    continue
    except Exception as e:
        raise TranscriptExtractionError(f"pytube ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    raise TranscriptExtractionError(
        f"pytube: ë§¤ì¹­ë˜ëŠ” ìë§‰ ì—†ìŒ (ì‚¬ìš©ê°€ëŠ¥: {list(available.keys()) if 'available' in locals() else 'N/A'})"
    )

# ---- yt-dlp (2ì°¨ í´ë°±, í—¤ë”ì „ë‹¬Â·ì¬ì‹œë„ ë³´ê°•) ----
def parse_vtt(vtt: str) -> List[str]:
    lines = []
    blocks = [b for b in vtt.strip().split("\n\n") if "-->" in b]
    for block in blocks:
        rows = block.split("\n")
        if not rows:
            continue
        ts = rows[0].replace(",", ".")
        m = re.match(r"(\d+):(\d+):(\d+(?:\.\d+)?)", ts)
        start = 0.0
        if m:
            h, m_, s = m.groups()
            start = int(h) * 3600 + int(m_) * 60 + float(s)
        text = " ".join(rows[1:]).strip()
        text = re.sub(r"<.*?>", " ", text)
        text = re.sub(r"\s+", " ", text)
        if text:
            lines.append(f"[{start:.1f}] {text}")
    return lines

def parse_srv3_json(json_data: str) -> List[str]:
    try:
        data = json.loads(json_data)
        lines = []
        for event in data.get("events", []):
            start_time = event.get("tStartMs", 0) / 1000.0
            segs = event.get("segs", [])
            text = "".join([seg.get("utf8", "") for seg in segs]).strip()
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        return lines
    except Exception:
        return []

def parse_ttml(ttml_data: str) -> List[str]:
    try:
        lines = []
        pattern = r'<p[^>]*begin="([^"]*)"[^>]*>(.*?)</p>'
        for match in re.finditer(pattern, ttml_data, re.DOTALL):
            time_str = match.group(1)
            text_content = match.group(2)
            try:
                parts = time_str.replace(",", ".").split(":")
                if len(parts) == 3:
                    h, m, s = parts
                    start_time = int(h) * 3600 + int(m) * 60 + float(s)
                else:
                    start_time = 0.0
            except:
                start_time = 0.0
            text = re.sub(r"<.*?>", " ", text_content)
            text = html.unescape(text)
            text = re.sub(r"\s+", " ", text).strip()
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        return lines
    except Exception:
        return []

def fetch_via_ytdlp_enhanced(url_or_id: str, langs: List[str]) -> str:
    url = to_clean_watch_url(url_or_id)
    common_headers = build_common_headers()
    ydl_opts = {
        "quiet": True,
        "no_warnings": True,
        "noplaylist": True,
        "writesubtitles": False,
        "writeautomaticsub": False,
        "socket_timeout": 60,
        "retries": 3,
        "http_headers": common_headers,  # ì¤‘ìš”: yt-dlp ì¸¡ ìš”ì²­ì—ë„ ë™ì¼ í—¤ë”
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
    except Exception as e:
        raise TranscriptExtractionError(f"yt-dlp ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

    subs = info.get("subtitles") or {}
    autos = info.get("automatic_captions") or {}

    candidates = []
    for lg in langs:
        if lg in subs:
            candidates.append(("manual", lg, subs[lg]))
    for lg in langs:
        if lg in autos:
            candidates.append(("auto", lg, autos[lg]))
    if "en" not in langs:
        if "en" in subs:
            candidates.append(("manual", "en", subs["en"]))
        if "en" in autos:
            candidates.append(("auto", "en", autos["en"]))
    if not candidates:
        all_available = list(subs.keys()) + list(autos.keys())
        if all_available:
            first_lang = all_available[0]
            if first_lang in subs:
                candidates.append(("manual", first_lang, subs[first_lang]))
            elif first_lang in autos:
                candidates.append(("auto", first_lang, autos[first_lang]))

    format_priority = ["vtt", "webvtt", "srv3", "ttml", "json3"]

    for kind, lg, fmt_list in candidates:
        if not fmt_list:
            continue
        sorted_formats = []
        for fmt_name in format_priority:
            for item in fmt_list:
                if item.get("ext", "").lower() == fmt_name:
                    sorted_formats.append(item)
        for item in fmt_list:
            if item not in sorted_formats:
                sorted_formats.append(item)

        for item in sorted_formats:
            try:
                data_bytes = urlopen_with_headers(item["url"], common_headers, timeout=30, retries=3)
                data = data_bytes.decode("utf-8", errors="ignore")
                ext = item.get("ext", "").lower()

                if ext in ("vtt", "webvtt"):
                    lines = parse_vtt(data)
                    if lines:
                        st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, {ext.upper()})")
                        return "\n".join(lines)
                elif ext in ("srv3", "json3"):
                    lines = parse_srv3_json(data)
                    if lines:
                        st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, SRV3)")
                        return "\n".join(lines)
                elif ext == "ttml":
                    lines = parse_ttml(data)
                    if lines:
                        st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, TTML)")
                        return "\n".join(lines)
                else:
                    text = re.sub(r"<.*?>", " ", data)
                    text = html.unescape(text)
                    text = re.sub(r"\s+", " ", text).strip()
                    if text and len(text) > 100:
                        st.success(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, {ext.upper()})")
                        return text
            except Exception:
                continue

    available_langs = list(set(list(subs.keys()) + list(autos.keys())))
    raise TranscriptExtractionError(f"yt-dlp: ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ (ì‚¬ìš©ê°€ëŠ¥: {available_langs})")

# ---- ìµœì¢… ë˜í¼ + ìºì‹œ ----
def fetch_transcript_resilient(url: str, video_id: str, langs: List[str]) -> str:
    # ì„¸ì…˜ ìºì‹œ
    if "transcript_cache" not in st.session_state:
        st.session_state.transcript_cache = {}

    cache_key = (video_id, tuple(langs))
    if cache_key in st.session_state.transcript_cache:
        st.caption("ìºì‹œ íˆíŠ¸: ì´ì „ì— ì¶”ì¶œí•œ ìë§‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return st.session_state.transcript_cache[cache_key]

    errors = []
    # 1) YTA
    try:
        text = fetch_via_yta_with_retry(video_id, langs)
        st.session_state.transcript_cache[cache_key] = text
        return text
    except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable) as e:
        errors.append(f"YTA: {str(e)}")
        sleep(0.6)
    except TranscriptExtractionError as e:
        errors.append(f"YTA: {str(e)}")
        sleep(0.6)
    except Exception as e:
        errors.append(f"YTA: {str(e)}")
        sleep(0.6)

    # 2) yt-dlp
    try:
        text = fetch_via_ytdlp_enhanced(url, langs)
        st.session_state.transcript_cache[cache_key] = text
        return text
    except TranscriptExtractionError as e:
        errors.append(f"yt-dlp: {str(e)}")
        sleep(0.6)
    except Exception as e:
        errors.append(f"yt-dlp: {str(e)}")
        sleep(0.6)

    # 3) pytube
    try:
        text = fetch_via_pytube(url, langs)
        st.session_state.transcript_cache[cache_key] = text
        return text
    except TranscriptExtractionError as e:
        errors.append(f"pytube: {str(e)}")
    except Exception as e:
        errors.append(f"pytube: {str(e)}")

    with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´", expanded=False):
        for i, err in enumerate(errors, 1):
            st.text(f"{i}. {err}")

    if any("429" in err or "too many requests" in err.lower() for err in errors):
        raise TranscriptExtractionError("YouTube ìš”ì²­ ì œí•œ (429/ì¼ì‹œ ì°¨ë‹¨) - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì˜ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•´ ë³´ì„¸ìš”.")
    if any("ìë§‰" in err and ("ì—†ìŒ" in err or "ì°¾ì„ ìˆ˜ ì—†ìŒ" in err) for err in errors):
        raise TranscriptExtractionError("ì´ ì˜ìƒì—ëŠ” ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤.")
    raise TranscriptExtractionError("ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ - ìœ„ì˜ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ---- Streamlit UI ----
st.set_page_config(page_title="YouTube ìë§‰ ì¶”ì¶œê¸°", layout="wide")
st.title("ğŸ¬ YouTube ìë§‰ ì¶”ì¶œê¸°")
st.caption("YouTube ì˜ìƒì˜ ìë§‰ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ê³µì‹ í‚¤ ì—†ì´ ì›¹ ì—”ë“œí¬ì¸íŠ¸ ê¸°ë°˜)")

with st.sidebar:
    st.header("ì„¤ì •")
    lang_pref = st.multiselect(
        "ì–¸ì–´ ìš°ì„ ìˆœìœ„ (ìœ„ì—ì„œë¶€í„° ì‹œë„)",
        ["ko", "en", "ja", "zh-Hans", "zh-Hant", "es", "fr", "de"],
        default=["ko", "en"],
        help="ì„ í˜¸í•œ ì–¸ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„í•©ë‹ˆë‹¤.",
    )
    show_meta = st.toggle("ì˜ìƒ ì œëª©/ê¸¸ì´ í‘œì‹œ (ìš”ì²­ ì¶”ê°€ ë°œìƒ)", value=False)

url = st.text_input(
    "YouTube ë§í¬",
    placeholder="https://www.youtube.com/watch?v=... ë˜ëŠ” https://youtu.be/...",
    help="YouTube ì˜ìƒì˜ URLì„ ì…ë ¥í•˜ì„¸ìš”.",
)

if st.button("ìë§‰ ì¶”ì¶œ", type="primary"):
    if not url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    clean_url = to_clean_watch_url(url.strip())
    vid = extract_video_id(clean_url)
    if not vid:
        st.error("ìœ íš¨í•œ YouTube ë§í¬ê°€ ì•„ë‹™ë‹ˆë‹¤. URLì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    st.info(f"ë¹„ë””ì˜¤ ID: {vid}")

    if show_meta:
        with st.spinner("ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            info = safe_get_youtube_info(clean_url)
            if info:
                length_min = int((info.length or 0) / 60) if info.length else 0
                st.info(f"**ì œëª©**: {info.title}  |  **ê¸¸ì´**: ì•½ {length_min}ë¶„")
            else:
                st.caption("ì˜ìƒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ - ìë§‰ ì¶”ì¶œì„ ê³„ì†í•©ë‹ˆë‹¤.")

    with st.spinner("ìë§‰ ì¶”ì¶œ ì¤‘..."):
        try:
            transcript_text = fetch_transcript_resilient(clean_url, vid, lang_pref)
        except TranscriptExtractionError as e:
            st.error(f"âŒ {str(e)}")
            st.stop()
        except (NoTranscriptFound, TranscriptsDisabled) as e:
            st.error(f"âŒ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            st.stop()
        except VideoUnavailable:
            st.error("âŒ ì˜ìƒì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë¹„ê³µê°œ, ì§€ì—­/ì—°ë ¹ ì œí•œ ë“±)")
            st.stop()
        except Exception as e:
            st.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            st.stop()

    st.success("ìë§‰ ì¶”ì¶œ ì™„ë£Œ!")

    col1, col2 = st.columns([1, 4])
    with col1:
        st.download_button(
            "ğŸ“„ ìë§‰ ë‹¤ìš´ë¡œë“œ (TXT)",
            data=transcript_text.encode("utf-8"),
            file_name=f"transcript_{vid}.txt",
            mime="text/plain",
        )
    with col2:
        st.caption(f"ì´ {len(transcript_text.split()):,}ê°œ ë‹¨ì–´")

    st.subheader("ğŸ“„ ì¶”ì¶œëœ ìë§‰")
    st.text_area("", value=transcript_text, height=500, help="ìë§‰ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ë³µì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

st.markdown("---")
st.caption(
    "ğŸ’¡ íŒ: ê³µìœ  í˜¸ìŠ¤íŒ…/ê³µìœ  IP í™˜ê²½ì—ì„œëŠ” ìš”ì²­ ì œí•œ(429)ì´ ë°œìƒí•  ìˆ˜ ìˆì–´ìš”. ë™ì¼ ì˜ìƒ ë°˜ë³µ ìš”ì²­ì€ ìºì‹œë˜ë©°, "
    "ì¼ë¶€ ì˜ìƒì€ ì €ì‘ê¶ŒÂ·ì—°ë ¹Â·ì§€ì—­ ì œí•œìœ¼ë¡œ ìë§‰ ì ‘ê·¼ì´ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)
