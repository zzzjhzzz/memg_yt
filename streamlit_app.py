import re
import random
from time import sleep
import html
from typing import Optional, List
from urllib.parse import urlparse, parse_qs
from urllib.request import urlopen, Request
import ssl
import hashlib

import streamlit as st
from youtube_transcript_api import (
    YouTubeTranscriptApi,
    NoTranscriptFound,
    TranscriptsDisabled,
    VideoUnavailable,
)
from pytube import YouTube
import yt_dlp

# ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤ ì •ì˜
class TranscriptExtractionError(Exception):
    """ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ì»¤ìŠ¤í…€ ì˜ˆì™¸"""
    pass

# SSL ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°
ssl._create_default_https_context = ssl._create_unverified_context

# ---------------------------------
# ë´‡ ì°¨ë‹¨ ìš°íšŒ ì„¤ì •
# ---------------------------------

# ë” ë‹¤ì–‘í•˜ê³  í˜„ì‹¤ì ì¸ User-Agent ëª©ë¡
USER_AGENTS = [
    # Chrome Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    # Chrome Mac
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    # Firefox
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0",
    # Safari
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15",
    # Edge
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0",
]

def get_realistic_headers():
    """ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ìœ ì‚¬í•œ í—¤ë” ìƒì„±"""
    ua = random.choice(USER_AGENTS)
    
    # User-Agentì— ë”°ë¥¸ ë¸Œë¼ìš°ì € íƒ€ì… ê²°ì •
    if "Chrome" in ua:
        browser_hints = {
            "sec-ch-ua": '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"' if "Windows" in ua else '"macOS"',
        }
    else:
        browser_hints = {}
    
    base_headers = {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0",
    }
    
    # ë¸Œë¼ìš°ì €ë³„ í—¤ë” ì¶”ê°€
    base_headers.update(browser_hints)
    
    return base_headers

def get_session_fingerprint():
    """ì„¸ì…˜ë³„ ê³ ìœ  ì‹ë³„ì ìƒì„± (IP ë³€ê²½ ì‹œë®¬ë ˆì´ì…˜ìš©)"""
    if 'session_id' not in st.session_state:
        st.session_state.session_id = hashlib.md5(str(random.random()).encode()).hexdigest()[:8]
    return st.session_state.session_id

def smart_delay(attempt: int = 0, base_delay: float = 1.0):
    """ì§€ëŠ¥ì  ëŒ€ê¸° (ì¸ê°„ê³¼ ìœ ì‚¬í•œ íŒ¨í„´)"""
    # ê¸°ë³¸ ëŒ€ê¸° + ì§€ìˆ˜ ë°±ì˜¤í”„ + ëœë¤ ì§€í„°
    delay = base_delay * (1.5 ** attempt) + random.uniform(0.5, 2.0)
    
    # ë„ˆë¬´ ê¸¸ë©´ ìµœëŒ€ê°’ìœ¼ë¡œ ì œí•œ
    delay = min(delay, 15.0)
    
    st.caption(f"â³ ìì—°ìŠ¤ëŸ¬ìš´ ê°„ê²©ìœ¼ë¡œ ëŒ€ê¸° ì¤‘... ({delay:.1f}ì´ˆ)")
    sleep(delay)

# ---------------------------------
# ìë§‰ ì¤‘ë³µ ì œê±° ë° ë³‘í•© í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ---------------------------------
def clean_duplicate_subtitles(transcript_text: str) -> str:
    """ìë§‰ì—ì„œ ì¤‘ë³µëœ ë¬¸ì¥ë“¤ì„ ì œê±°"""
    lines = transcript_text.strip().split('\n')
    cleaned_lines = []
    seen_texts = set()
    
    for line in lines:
        if not line.strip():
            continue
            
        # ì‹œê°„ íƒœê·¸ì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬
        match = re.match(r'\[(\d+\.?\d*)\]\s*(.*)', line)
        if not match:
            continue
            
        timestamp = float(match.group(1))
        text = match.group(2).strip()
        
        if not text or text in ['[Music]', '[Applause]', '[Laughter]']:
            continue
            
        # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì²´í¬ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨)
        text_lower = text.lower()
        
        # ì™„ì „ ì¤‘ë³µ ì œê±°
        if text_lower in seen_texts:
            continue
            
        # ë¶€ë¶„ ì¤‘ë³µ ì œê±° (í•œ ë¬¸ì¥ì´ ë‹¤ë¥¸ ë¬¸ì¥ì— í¬í•¨ëœ ê²½ìš°)
        is_duplicate = False
        texts_to_remove = []
        
        for seen_text in list(seen_texts):
            # í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ì´ì „ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ê±°ë‚˜ ê·¸ ë°˜ëŒ€
            if text_lower in seen_text:
                # í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ë” ì§§ìœ¼ë©´ ìŠ¤í‚µ
                is_duplicate = True
                break
            elif seen_text in text_lower:
                # ì´ì „ í…ìŠ¤íŠ¸ê°€ ë” ì§§ìœ¼ë©´ ì œê±° ëŒ€ìƒìœ¼ë¡œ ë§ˆí‚¹
                texts_to_remove.append(seen_text)
                
        if not is_duplicate:
            # ì œê±°í•  í…ìŠ¤íŠ¸ë“¤ ì²˜ë¦¬
            for old_text in texts_to_remove:
                seen_texts.discard(old_text)
            
            seen_texts.add(text_lower)
            cleaned_lines.append(f"[{timestamp:.1f}] {text}")
    
    return '\n'.join(cleaned_lines)

def merge_consecutive_subtitles(transcript_text: str, time_threshold: float = 2.0) -> str:
    """ì—°ì†ëœ ë¹„ìŠ·í•œ ìë§‰ë“¤ì„ ë³‘í•©"""
    lines = transcript_text.strip().split('\n')
    merged_lines = []
    
    i = 0
    while i < len(lines):
        if not lines[i].strip():
            i += 1
            continue
            
        match = re.match(r'\[(\d+\.?\d*)\]\s*(.*)', lines[i])
        if not match:
            i += 1
            continue
            
        current_time = float(match.group(1))
        current_text = match.group(2).strip()
        
        # ë‹¤ìŒ ë¼ì¸ë“¤ê³¼ ë¹„êµí•´ì„œ ë³‘í•© ê°€ëŠ¥í•œì§€ ì²´í¬
        merged_text = current_text
        j = i + 1
        
        while j < len(lines):
            if j >= len(lines):
                break
                
            next_match = re.match(r'\[(\d+\.?\d*)\]\s*(.*)', lines[j])
            if not next_match:
                j += 1
                continue
                
            next_time = float(next_match.group(1))
            next_text = next_match.group(2).strip()
            
            # ì‹œê°„ì´ ë„ˆë¬´ ë©€ë©´ ì¤‘ë‹¨
            if (next_time - current_time) > time_threshold:
                break
                
            # í…ìŠ¤íŠ¸ê°€ í˜„ì¬ í…ìŠ¤íŠ¸ì˜ ì—°ì¥ì¸ì§€ ì²´í¬
            if (current_text.lower() in next_text.lower() or 
                next_text.lower() in current_text.lower()):
                # ë” ê¸´ í…ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
                if len(next_text) > len(merged_text):
                    merged_text = next_text
                j += 1
            else:
                break
                
        merged_lines.append(f"[{current_time:.1f}] {merged_text}")
        i = max(i + 1, j)
    
    return '\n'.join(merged_lines)

def apply_subtitle_cleaning(raw_transcript: str, clean_duplicates: bool, merge_consecutive: bool) -> str:
    """ì‚¬ìš©ì ì„¤ì •ì— ë”°ë¼ ìë§‰ ì •ë¦¬ ì ìš©"""
    result = raw_transcript
    
    if clean_duplicates:
        result = clean_duplicate_subtitles(result)
    
    if merge_consecutive:
        result = merge_consecutive_subtitles(result)
    
    return result

# ---------------------------------
# URL ì •ë¦¬ / ë¹„ë””ì˜¤ID ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
# ---------------------------------
YOUTUBE_URL_RE = re.compile(
    r'(?:https?://)?(?:www\.)?(?:youtube\.com/(?:watch\?v=|embed/|live/|shorts/)|youtu\.be/)([\w-]{11})(?:\S+)?'
)

def extract_video_id(url: str) -> Optional[str]:
    if not url:
        return None
    
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë¨¼ì € ì‹œë„
    m = YOUTUBE_URL_RE.search(url)
    if m:
        return m.group(1)
    
    # URL íŒŒì‹±ìœ¼ë¡œ ì¬ì‹œë„
    try:
        parsed = urlparse(url)
        if parsed.hostname in ['youtube.com', 'www.youtube.com']:
            vid = parse_qs(parsed.query).get("v", [None])[0]
            if vid and len(vid) == 11:
                return vid
        elif parsed.hostname in ['youtu.be', 'www.youtu.be']:
            vid = parsed.path.lstrip('/')
            if len(vid) == 11:
                return vid
    except Exception:
        pass
    
    return None

def to_clean_watch_url(url_or_id: str) -> str:
    """ì§§ì€ ì£¼ì†Œ/íŒŒë¼ë¯¸í„°ë¥¼ í‘œì¤€ watch URLë¡œ ì •ë¦¬."""
    vid = extract_video_id(url_or_id) if "http" in url_or_id else url_or_id
    return f"https://www.youtube.com/watch?v={vid}" if vid else url_or_id

# ---------------------------------
# í–¥ìƒëœ ìë§‰ ì¶”ì¶œ í•¨ìˆ˜ë“¤
# ---------------------------------
def fetch_via_yta_with_enhanced_retry(video_id: str, langs: List[str], max_retries: int = 3) -> str:
    """í–¥ìƒëœ ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ YTA ìë§‰ ì¶”ì¶œ"""
    last_error = None
    session_id = get_session_fingerprint()
    
    for attempt in range(max_retries):
        try:
            # ê° ì‹œë„ë§ˆë‹¤ ì•½ê°„ì˜ ì§€ì—°
            if attempt > 0:
                smart_delay(attempt, 2.0)
            
            # ì„¸ì…˜ ìƒíƒœ í‘œì‹œ
            st.caption(f"ğŸ”„ YTA ì‹œë„ {attempt + 1}/{max_retries} (ì„¸ì…˜: {session_id})")
            
            tl = YouTubeTranscriptApi.list_transcripts(video_id)
            
            try:
                tr = tl.find_transcript(langs)
            except Exception:
                tr = tl.find_generated_transcript(langs)
            
            entries = tr.fetch()
            st.success(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ (YTA): {tr.language}" + (" [ìë™ìƒì„±]" if tr.is_generated else " [ìˆ˜ë™]"))
            return "\n".join([f"[{e['start']:.1f}] {e['text']}" for e in entries])
            
        except Exception as e:
            last_error = e
            error_msg = str(e).lower()
            
            # íŠ¹ì • ì˜¤ë¥˜ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
            if any(phrase in error_msg for phrase in ["too many requests", "429", "rate limit"]):
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) + random.uniform(3, 8)
                    st.warning(f"âš ï¸ API ìš”ì²­ ì œí•œ ê°ì§€. {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    sleep(wait_time)
                    continue
                else:
                    raise TranscriptExtractionError(f"YouTube API ìš”ì²­ ì œí•œ ì´ˆê³¼")
            elif any(phrase in error_msg for phrase in ["403", "forbidden", "blocked"]):
                # IP ì°¨ë‹¨ì˜ ê²½ìš° ë” ê¸´ ëŒ€ê¸°
                if attempt < max_retries - 1:
                    wait_time = 10 + random.uniform(5, 15)
                    st.warning(f"ğŸš« ì ‘ê·¼ ì°¨ë‹¨ ê°ì§€. {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    sleep(wait_time)
                    continue
                else:
                    raise TranscriptExtractionError(f"YouTubeì—ì„œ ì ‘ê·¼ì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤")
            else:
                # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¬ì‹œë„ ì—†ì´ ë°”ë¡œ ë°œìƒ
                if isinstance(e, (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable)):
                    raise
                else:
                    raise TranscriptExtractionError(f"YTA ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    raise TranscriptExtractionError(f"YTA ì¬ì‹œë„ ì‹¤íŒ¨: {str(last_error)}")

def safe_get_youtube_info_enhanced(url: str):
    """í–¥ìƒëœ ì•ˆì „í•œ YouTube ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        headers = get_realistic_headers()
        
        ydl_opts = {
            "quiet": True,
            "noplaylist": True,
            "extract_flat": False,
            "http_headers": headers,
            "socket_timeout": 30,
            "retries": 2,
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            
        class YouTubeInfo:
            def __init__(self, info_dict):
                self.title = info_dict.get('title', 'ì œëª© í™•ì¸ ë¶ˆê°€')
                self.length = info_dict.get('duration', 0)
                
        return YouTubeInfo(info)
        
    except Exception:
        return None

def fetch_via_ytdlp_enhanced_stealth(url_or_id: str, langs: List[str]) -> str:
    """ìŠ¤í…”ìŠ¤ ëª¨ë“œ yt-dlp ìë§‰ ê°€ì ¸ì˜¤ê¸°"""
    url = to_clean_watch_url(url_or_id)
    headers = get_realistic_headers()
    session_id = get_session_fingerprint()
    
    # ë” í˜„ì‹¤ì ì¸ yt-dlp ì„¤ì •
    ydl_opts = {
        "quiet": True,
        "no_warnings": True,
        "noplaylist": True,
        "writesubtitles": False,
        "writeautomaticsub": False,
        "socket_timeout": 45,
        "retries": 2,
        "http_headers": headers,
        # YouTube ìš°íšŒë¥¼ ìœ„í•œ ì¶”ê°€ ì˜µì…˜ë“¤
        "extractor_args": {
            "youtube": {
                "skip": ["dash", "hls"],
                "player_client": ["android", "web"],
            }
        },
        # ì¿ í‚¤ ë° ìºì‹œ ì„¤ì •
        "cachedir": False,
        "no_cache_dir": True,
    }
    
    st.caption(f"ğŸ” yt-dlp ìŠ¤í…”ìŠ¤ ëª¨ë“œ (ì„¸ì…˜: {session_id})")
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
    except Exception as e:
        raise TranscriptExtractionError(f"yt-dlp ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

    subs = info.get("subtitles") or {}
    autos = info.get("automatic_captions") or {}
    
    candidates = []
    
    # ìš°ì„ ìˆœìœ„: ìˆ˜ë™ ìë§‰ > ìë™ ìë§‰
    for lg in langs:
        if lg in subs:
            candidates.append(("manual", lg, subs[lg]))
    
    for lg in langs:
        if lg in autos:
            candidates.append(("auto", lg, autos[lg]))
    
    # ì˜ì–´ í´ë°±
    if "en" not in langs:
        if "en" in subs:
            candidates.append(("manual", "en", subs["en"]))
        if "en" in autos:
            candidates.append(("auto", "en", autos["en"]))
    
    # ì•„ë¬´ ì–¸ì–´ë‚˜ ì‚¬ìš©
    if not candidates:
        all_available = list(subs.keys()) + list(autos.keys())
        if all_available:
            first_lang = all_available[0]
            if first_lang in subs:
                candidates.append(("manual", first_lang, subs[first_lang]))
            elif first_lang in autos:
                candidates.append(("auto", first_lang, autos[first_lang]))

    format_priority = ["vtt", "webvtt", "srv3", "ttml", "json3"]
    
    for kind, lg, fmt_list in candidates:
        if not fmt_list:
            continue
        
        # í¬ë§· ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬
        sorted_formats = []
        for fmt_name in format_priority:
            for item in fmt_list:
                if item.get("ext", "").lower() == fmt_name:
                    sorted_formats.append(item)
        
        for item in fmt_list:
            if item not in sorted_formats:
                sorted_formats.append(item)
        
        for item in sorted_formats:
            try:
                # í–¥ìƒëœ í—¤ë”ë¡œ ìš”ì²­
                req = Request(item["url"], headers=headers)
                
                # ì‘ì€ ëœë¤ ì§€ì—° ì¶”ê°€
                sleep(random.uniform(0.5, 1.5))
                
                with urlopen(req, timeout=30) as resp:
                    data = resp.read().decode("utf-8", errors="ignore")
                
                ext = item.get("ext", "").lower()
                
                if ext in ("vtt", "webvtt"):
                    lines = parse_vtt(data)
                    if lines:
                        st.success(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, {ext.upper()})")
                        return "\n".join(lines)
                
                elif ext == "srv3":
                    lines = parse_srv3_json(data)
                    if lines:
                        st.success(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, SRV3)")
                        return "\n".join(lines)
                
                elif ext == "ttml":
                    lines = parse_ttml(data)
                    if lines:
                        st.success(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, TTML)")
                        return "\n".join(lines)
                        
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                    text = re.sub(r"<.*?>", " ", data)
                    text = html.unescape(text)
                    text = re.sub(r"\s+", " ", text).strip()
                    if text and len(text) > 100:
                        st.success(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ (yt-dlp): {lg} ({kind}, {ext.upper()})")
                        return text
                        
            except Exception as e:
                st.caption(f"âš ï¸ {ext.upper()} í¬ë§· ì‹¤íŒ¨: {str(e)[:50]}...")
                continue

    available_langs = list(set(list(subs.keys()) + list(autos.keys())))
    raise TranscriptExtractionError(f"yt-dlp: ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ (ì‚¬ìš©ê°€ëŠ¥: {available_langs})")

def fetch_via_pytube_enhanced(url_or_id: str, langs: List[str]) -> str:
    """í–¥ìƒëœ pytube ìë§‰ ì¶”ì¶œ"""
    url = to_clean_watch_url(url_or_id)
    session_id = get_session_fingerprint()
    
    st.caption(f"ğŸ” pytube í–¥ìƒ ëª¨ë“œ (ì„¸ì…˜: {session_id})")
    
    try:
        # User-Agent ì„¤ì •ìœ¼ë¡œ pytube ì´ˆê¸°í™”
        import urllib.request
        opener = urllib.request.build_opener()
        headers = get_realistic_headers()
        opener.addheaders = [(k, v) for k, v in headers.items()]
        urllib.request.install_opener(opener)
        
        # ì²« ë²ˆì§¸ ì‹œë„
        try:
            yt = YouTube(url, use_oauth=False, allow_oauth_cache=False)
            _ = yt.title  # ë©”íƒ€ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸
        except Exception:
            # ì¬ì‹œë„ with ë‹¤ë¥¸ í—¤ë”
            smart_delay(0, 1.0)
            headers = get_realistic_headers()
            opener = urllib.request.build_opener()
            opener.addheaders = [(k, v) for k, v in headers.items()]
            urllib.request.install_opener(opener)
            
            yt = YouTube(url, use_oauth=False, allow_oauth_cache=False)
            _ = yt.title
        
        tracks = yt.captions
        if not tracks:
            raise TranscriptExtractionError("pytube: ìë§‰ íŠ¸ë™ì´ ì—†ìŒ")

        # ì–¸ì–´ ìš°ì„ ìˆœìœ„ ì„¤ì •
        candidates = []
        for lg in langs:
            candidates.append(lg)
            candidates.append(f"a.{lg}")  # ìë™ìƒì„± ìë§‰
        
        if "en" not in [c.replace("a.", "") for c in candidates]:
            candidates.extend(["en", "a.en"])

        available_codes = {c.code: c for c in tracks}

        for code in candidates:
            cap = available_codes.get(code)
            
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            if not cap:
                for k, v in available_codes.items():
                    if k.lower().startswith(code.lower().replace("a.", "")):
                        cap = v
                        code = k
                        break
            
            if not cap:
                continue

            try:
                # SRT ë°©ì‹ ë¨¼ì € ì‹œë„
                srt = cap.generate_srt_captions()
                lines = []
                
                for block in srt.strip().split("\n\n"):
                    if not block.strip():
                        continue
                        
                    parts = block.split("\n")
                    if len(parts) >= 3:
                        ts = parts[1].split("-->")[0].strip()
                        try:
                            h, m, s_ms = ts.split(":")
                            s, ms = s_ms.split(",")
                            start = int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0
                            text = " ".join(parts[2:]).strip()
                            if text:
                                lines.append(f"[{start:.1f}] {text}")
                        except (ValueError, IndexError):
                            continue
                
                if lines:
                    st.success(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ (pytube): {code}")
                    return "\n".join(lines)
                    
            except Exception:
                # XML ë°©ì‹ìœ¼ë¡œ í´ë°±
                try:
                    xml = cap.xml_captions
                    items = clean_xml_text(xml)
                    if items:
                        st.success(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ (pytube): {code}")
                        return "\n".join([f"[{stt:.1f}] {txt}" for stt, txt in items])
                except Exception:
                    continue

    except Exception as e:
        raise TranscriptExtractionError(f"pytube ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    raise TranscriptExtractionError(f"pytube: ë§¤ì¹­ë˜ëŠ” ìë§‰ ì—†ìŒ")

# ê¸°ì¡´ íŒŒì‹± í•¨ìˆ˜ë“¤ (parse_vtt, parse_srv3_json, parse_ttml, clean_xml_text)ì€ ë™ì¼

def parse_vtt(vtt: str) -> List[str]:
    """WebVTTë¥¼ [start] text í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
    lines = []
    blocks = [b for b in vtt.strip().split("\n\n") if "-->" in b]
    
    for block in blocks:
        rows = block.split("\n")
        if not rows:
            continue
            
        ts = rows[0]
        m = re.match(r"(\d+):(\d+):(\d+(?:\.\d+)?)", ts.replace(",", "."))
        
        start = 0.0
        if m:
            h, m_, s = m.groups()
            start = int(h) * 3600 + int(m_) * 60 + float(s)
        
        text = " ".join(rows[1:]).strip()
        text = re.sub(r"<.*?>", " ", text)
        text = re.sub(r"\s+", " ", text)
        if text:
            lines.append(f"[{start:.1f}] {text}")
    
    return lines

def parse_srv3_json(json_data: str) -> List[str]:
    """YouTube SRV3 JSON ìë§‰ íŒŒì‹±"""
    try:
        import json
        data = json.loads(json_data)
        lines = []
        
        events = data.get("events", [])
        for event in events:
            start_time = event.get("tStartMs", 0) / 1000.0
            segs = event.get("segs", [])
            text = "".join([seg.get("utf8", "") for seg in segs]).strip()
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        
        return lines
    except Exception:
        return []

def parse_ttml(ttml_data: str) -> List[str]:
    """TTML XML ìë§‰ íŒŒì‹±"""
    try:
        lines = []
        pattern = r'<p[^>]*begin="([^"]*)"[^>]*>(.*?)</p>'
        
        for match in re.finditer(pattern, ttml_data, re.DOTALL):
            time_str = match.group(1)
            text_content = match.group(2)
            
            try:
                parts = time_str.replace(',', '.').split(':')
                if len(parts) == 3:
                    h, m, s = parts
                    start_time = int(h) * 3600 + int(m) * 60 + float(s)
                else:
                    start_time = 0.0
            except:
                start_time = 0.0
            
            text = re.sub(r"<.*?>", " ", text_content)
            text = html.unescape(text)
            text = re.sub(r"\s+", " ", text).strip()
            
            if text:
                lines.append(f"[{start_time:.1f}] {text}")
        
        return lines
    except Exception:
        return []

def clean_xml_text(xml_text: str) -> List[tuple]:
    """XMLì—ì„œ (start, text) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜."""
    items = []
    xml_text = xml_text.replace("\n", "")
    pattern = r'<text[^>]*start="([\d\.]+)"[^>]*>(.*?)</text>'
    
    for m in re.finditer(pattern, xml_text, re.DOTALL):
        try:
            start = float(m.group(1))
            raw = re.sub(r"<.*?>", " ", m.group(2))
            text = html.unescape(raw)
            text = re.sub(r"\s+", " ", text).strip()
            if text:
                items.append((start, text))
        except ValueError:
            continue
    return items

def fetch_transcript_resilient_enhanced(url: str, video_id: str, langs: List[str]) -> str:
    """í–¥ìƒëœ 3ë‹¨ê³„ í´ë°±ìœ¼ë¡œ ìë§‰ ê°€ì ¸ì˜¤ê¸°"""
    errors = []
    method_results = []
    session_id = get_session_fingerprint()
    
    st.info(f"ğŸ¯ ìë§‰ ì¶”ì¶œ ì‹œì‘ (ì„¸ì…˜: {session_id}, ì–¸ì–´: {', '.join(langs)})")
    
    # ì„¸ì…˜ ê¸°ë°˜ ë°©ë²• ìˆœì„œ ëœë¤í™”
    methods = ["yta", "ytdlp", "pytube"]
    if int(session_id[-1], 16) % 2 == 0:  # ì„¸ì…˜ ID ê¸°ë°˜ìœ¼ë¡œ ìˆœì„œ ë³€ê²½
        methods = ["ytdlp", "yta", "pytube"]
    
    for i, method in enumerate(methods):
        if i > 0:
            smart_delay(i-1, 3.0)  # ë°©ë²• ê°„ ì§€ì—°
        
        st.write(f"ğŸ”„ **ë°©ë²• {i+1}/3**: {method.upper()} ì‹œë„ ì¤‘...")
        
        try:
            if method == "yta":
                result = fetch_via_yta_with_enhanced_retry(video_id, langs, max_retries)
            elif method == "ytdlp":
                result = fetch_via_ytdlp_enhanced_stealth(url, langs)
            elif method == "pytube":
                result = fetch_via_pytube_enhanced(url, langs)
            
            if result and len(result.strip()) > 0:
                st.write(f"âœ… **{method.upper()} ì„±ê³µ**: {len(result)} ë¬¸ì ì¶”ì¶œ")
                return result
            else:
                st.write(f"âš ï¸ {method.upper()} ë¹ˆ ê²°ê³¼")
                method_results.append((method.upper(), "ë¹ˆ ê²°ê³¼"))
                
        except TranscriptExtractionError as e:
            st.write(f"âŒ {method.upper()} ì‹¤íŒ¨: {str(e)}")
            method_results.append((method.upper(), f"ì‹¤íŒ¨: {str(e)}"))
            errors.append(f"{method.upper()}: {str(e)}")
        except (NoTranscriptFound, TranscriptsDisabled) as e:
            st.write(f"âŒ {method.upper()} ìë§‰ ì—†ìŒ: {str(e)}")
            method_results.append((method.upper(), f"ìë§‰ ì—†ìŒ: {str(e)}"))
            errors.append(f"{method.upper()}: {str(e)}")
        except VideoUnavailable as e:
            st.write(f"âŒ {method.upper()} ì˜ìƒ ì ‘ê·¼ ë¶ˆê°€: {str(e)}")
            method_results.append((method.upper(), f"ì˜ìƒ ì ‘ê·¼ ë¶ˆê°€: {str(e)}"))
            errors.append(f"{method.upper()}: ì˜ìƒ ì ‘ê·¼ ë¶ˆê°€ - {str(e)}")
            # ì˜ìƒ ì ‘ê·¼ ë¶ˆê°€ë©´ ë‹¤ë¥¸ ë°©ë²•ë„ ì‹¤íŒ¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            break
        except Exception as e:
            st.write(f"âŒ {method.upper()} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            method_results.append((method.upper(), f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"))
            errors.append(f"{method.upper()}: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - {str(e)}")

    # ì‹¤íŒ¨ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­
    st.error("ğŸš« **ëª¨ë“  ë°©ë²• ì‹¤íŒ¨**")
    
    with st.expander("ğŸ“Š ìƒì„¸ ì‹¤íŒ¨ ë¶„ì„", expanded=True):
        for i, (method, error) in enumerate(method_results, 1):
            st.text(f"{i}. {method}: {error}")
    
    # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ë° í•´ê²°ì±… ì œì•ˆ
    all_errors_text = " ".join(errors).lower()
    
    st.subheader("ğŸ”§ ê¶Œì¥ í•´ê²°ì±…")
    
    if any(phrase in all_errors_text for phrase in ["429", "too many requests", "rate limit"]):
        st.warning("**ì›ì¸**: YouTube API ìš”ì²­ ì œí•œ")
        st.markdown("""
        **í•´ê²°ì±…**:
        - 5-10ë¶„ í›„ ë‹¤ì‹œ ì‹œë„
        - VPN ì‚¬ìš©í•˜ì—¬ IP ë³€ê²½
        - ë‹¤ë¥¸ ì‹œê°„ëŒ€ì— ì‹œë„
        - ì—¬ëŸ¬ ì˜ìƒì„ ì—°ì†ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ë§ê³  ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
        """)
        raise TranscriptExtractionError("YouTube API ìš”ì²­ ì œí•œ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
        
    elif any(phrase in all_errors_text for phrase in ["403", "forbidden", "blocked", "400", "bad request"]):
        st.warning("**ì›ì¸**: IP/ë´‡ ì°¨ë‹¨")
        st.markdown("""
        **í•´ê²°ì±…**:
        - VPNìœ¼ë¡œ ë‹¤ë¥¸ êµ­ê°€ IP ì‚¬ìš©
        - ëª¨ë°”ì¼ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
        - ì‹œí¬ë¦¿/í”„ë¼ì´ë¹— ë¸Œë¼ìš°ì €ì—ì„œ ì˜ìƒ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        - ë‹¤ë¥¸ ì‹œê°„ëŒ€ì— ì¬ì‹œë„
        """)
        raise TranscriptExtractionError("YouTubeì—ì„œ ì ‘ê·¼ì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤ - VPN ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
        
    elif any(phrase in all_errors_text for phrase in ["subtitles are disabled", "no transcript found", "ìë§‰ ì—†ìŒ"]):
        st.info("**ì›ì¸**: ìë§‰ ë¹„í™œì„±í™”")
        st.markdown("""
        **í™•ì¸ì‚¬í•­**:
        - í•´ë‹¹ ì˜ìƒì— ì‹¤ì œë¡œ ìë§‰ì´ ìˆëŠ”ì§€ YouTubeì—ì„œ ì§ì ‘ í™•ì¸
        - ìë™ìƒì„± ìë§‰ë„ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        - ë‹¤ë¥¸ ì–¸ì–´ì˜ ìë§‰ì´ ìˆëŠ”ì§€ í™•ì¸
        """)
        raise TranscriptExtractionError("ì´ ì˜ìƒì—ëŠ” ìë§‰ì´ ì—†ê±°ë‚˜ ìë§‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
    elif any(phrase in all_errors_text for phrase in ["ì˜ìƒ ì ‘ê·¼ ë¶ˆê°€", "video unavailable", "private"]):
        st.info("**ì›ì¸**: ì˜ìƒ ì ‘ê·¼ ì œí•œ")
        st.markdown("""
        **í™•ì¸ì‚¬í•­**:
        - ì˜ìƒì´ ë¹„ê³µê°œ ì„¤ì •ì¸ì§€ í™•ì¸
        - ì—°ë ¹ ì œí•œì´ ìˆëŠ”ì§€ í™•ì¸
        - ì§€ì—­ ì œí•œì´ ìˆëŠ”ì§€ í™•ì¸
        - ì˜ìƒì´ ì‚­ì œë˜ì—ˆëŠ”ì§€ í™•ì¸
        """)
        raise TranscriptExtractionError("ì˜ìƒì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë¹„ê³µê°œ, ì—°ë ¹ì œí•œ, ì§€ì—­ì œí•œ ë“±)")
        
    else:
        st.warning("**ì›ì¸**: ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
        st.markdown("""
        **ì¼ë°˜ì  í•´ê²°ì±…**:
        - ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
        - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„
        - ë‹¤ë¥¸ ë¸Œë¼ìš°ì €ë‚˜ í™˜ê²½ì—ì„œ ì‹œë„
        - YouTubeì—ì„œ í•´ë‹¹ ì˜ìƒ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸
        """)
        raise TranscriptExtractionError("ì•Œ ìˆ˜ ì—†ëŠ” ì´ìœ ë¡œ ìë§‰ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

# ---------------------------------
# Streamlit UI (í–¥ìƒëœ ë²„ì „)
# ---------------------------------
st.set_page_config(page_title="YouTube ìë§‰ ì¶”ì¶œê¸° (Anti-Bot)", layout="wide")
st.title("ğŸ¬ YouTube ìë§‰ ì¶”ì¶œê¸°")
st.caption("YouTube ì˜ìƒì˜ ìë§‰ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë´‡ ì°¨ë‹¨ ìš°íšŒ ê¸°ëŠ¥ í¬í•¨.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'extraction_count' not in st.session_state:
    st.session_state.extraction_count = 0
if 'last_extraction_time' not in st.session_state:
    st.session_state.last_extraction_time = 0

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # ì„¸ì…˜ ì •ë³´ í‘œì‹œ
    session_id = get_session_fingerprint()
    st.info(f"ì„¸ì…˜ ID: {session_id}")
    st.caption(f"ì¶”ì¶œ íšŸìˆ˜: {st.session_state.extraction_count}")
    
    lang_pref = st.multiselect(
        "ì–¸ì–´ ìš°ì„ ìˆœìœ„ (ìœ„ì—ì„œë¶€í„° ì‹œë„)",
        ["ko", "en", "ja", "zh-Hans", "zh-Hant", "es", "fr", "de"],
        default=["ko", "en"],
        help="ì„ í˜¸í•˜ëŠ” ì–¸ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì„ íƒí•˜ì„¸ìš”"
    )
    
    show_meta = st.toggle("ì˜ìƒ ì œëª©/ê¸¸ì´ í‘œì‹œ", value=True)
    
    st.subheader("ğŸ§¹ ìë§‰ ì •ë¦¬ ì˜µì…˜")
    clean_duplicates = st.toggle(
        "ì¤‘ë³µ ìë§‰ ì œê±°", 
        value=True,
        help="ê°™ì€ ë‚´ìš©ì´ ë°˜ë³µë˜ëŠ” ìë§‰ì„ ì œê±°í•©ë‹ˆë‹¤"
    )
    merge_consecutive = st.toggle(
        "ì—°ì† ìë§‰ ë³‘í•©", 
        value=True,
        help="ë¹„ìŠ·í•œ ì‹œê°„ëŒ€ì˜ ìœ ì‚¬í•œ ìë§‰ì„ ë³‘í•©í•©ë‹ˆë‹¤"
    )
    
    st.subheader("ğŸ“¤ ì¶œë ¥ ì˜µì…˜")
    show_original = st.toggle(
        "ì›ë³¸ ìë§‰ë„ í•¨ê»˜ í‘œì‹œ", 
        value=False,
        help="ì •ë¦¬ëœ ìë§‰ê³¼ ì›ë³¸ ìë§‰ì„ ëª¨ë‘ í‘œì‹œí•©ë‹ˆë‹¤"
    )
    
    # ì°¨ë‹¨ ìš°íšŒ ì˜µì…˜
    st.subheader("ğŸ›¡ï¸ ì°¨ë‹¨ ìš°íšŒ ì„¤ì •")
    base_delay = st.slider(
        "ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)", 
        min_value=0.5, 
        max_value=5.0, 
        value=2.0,
        help="ìš”ì²­ ê°„ ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„"
    )
    
    max_retries = st.slider(
        "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜", 
        min_value=1, 
        max_value=5, 
        value=3,
        help="ê° ë°©ë²•ë³„ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜"
    )

# ë©”ì¸ ì…ë ¥
url = st.text_input(
    "ğŸ”— YouTube ë§í¬", 
    placeholder="https://www.youtube.com/watch?v=... ë˜ëŠ” https://youtu.be/...",
    help="YouTube ì˜ìƒì˜ URLì„ ì…ë ¥í•˜ì„¸ìš”"
)

# ì¶”ì¶œ íšŸìˆ˜ ì œí•œ ê²½ê³ 
if st.session_state.extraction_count >= 10:
    st.warning("âš ï¸ ë§ì€ ì¶”ì¶œì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. IP ì°¨ë‹¨ ìœ„í—˜ì´ ìˆìœ¼ë‹ˆ ì ì‹œ íœ´ì‹ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")

run = st.button("ğŸš€ ìë§‰ ì¶”ì¶œ", type="primary")

if run:
    if not url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    # ìš”ì²­ ì œí•œ ì²´í¬
    import time
    current_time = time.time()
    if current_time - st.session_state.last_extraction_time < 10:
        remaining = 10 - (current_time - st.session_state.last_extraction_time)
        st.warning(f"â° ìš”ì²­ ì œí•œ: {remaining:.1f}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        st.stop()

    clean_url = to_clean_watch_url(url.strip())
    vid = extract_video_id(clean_url)
    
    if not vid:
        st.error("âŒ ìœ íš¨í•œ YouTube ë§í¬ê°€ ì•„ë‹™ë‹ˆë‹¤. URLì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    st.info(f"ğŸ¯ ë¹„ë””ì˜¤ ID: `{vid}`")

    # ì¶”ì¶œ íšŸìˆ˜ ì—…ë°ì´íŠ¸
    st.session_state.extraction_count += 1
    st.session_state.last_extraction_time = current_time

    # ë©”íƒ€ ì •ë³´ í‘œì‹œ
    if show_meta:
        with st.spinner("ğŸ“‹ ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            try:
                info = safe_get_youtube_info_enhanced(clean_url)
                if info:
                    title = info.title
                    length_min = int((info.length or 0) / 60) if info.length else 0
                    st.success(f"**ğŸ“¹ ì œëª©**: {title}")
                    st.info(f"**â±ï¸ ê¸¸ì´**: ì•½ {length_min}ë¶„")
                else:
                    st.caption("ì˜ìƒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ - ìë§‰ ì¶”ì¶œì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception:
                st.caption("ì˜ìƒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ - ìë§‰ ì¶”ì¶œì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

    # ìë§‰ ì¶”ì¶œ
    with st.spinner("ğŸ” ìë§‰ ì¶”ì¶œ ì¤‘..."):
        try:
            raw_transcript = fetch_transcript_resilient_enhanced(clean_url, vid, lang_pref)
        except TranscriptExtractionError as e:
            st.error(f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            st.stop()
        except (NoTranscriptFound, TranscriptsDisabled) as e:
            st.error(f"ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            st.stop()
        except VideoUnavailable:
            st.error("ì˜ìƒì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë¹„ê³µê°œ, ì§€ì—­ì œí•œ, ì—°ë ¹ì œí•œ ë“±)")
            st.stop()
        except Exception as e:
            st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            st.stop()

    # ìë§‰ ì •ë¦¬ ì ìš©
    if clean_duplicates or merge_consecutive:
        with st.spinner("ğŸ§¹ ìë§‰ ì •ë¦¬ ì¤‘..."):
            cleaned_transcript = apply_subtitle_cleaning(raw_transcript, clean_duplicates, merge_consecutive)
    else:
        cleaned_transcript = raw_transcript

    # ê²°ê³¼ ì¶œë ¥
    st.success("ğŸ‰ ìë§‰ ì¶”ì¶œ ì™„ë£Œ!")
    
    # í†µê³„ ì •ë³´
    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        raw_word_count = len(raw_transcript.split())
        raw_lines = len([l for l in raw_transcript.split('\n') if l.strip()])
        st.metric("ì›ë³¸", f"{raw_word_count:,}ê°œ ë‹¨ì–´", f"{raw_lines}ì¤„")
    
    with col2:
        if cleaned_transcript != raw_transcript:
            cleaned_word_count = len(cleaned_transcript.split())
            cleaned_lines = len([l for l in cleaned_transcript.split('\n') if l.strip()])
            word_reduction = raw_word_count - cleaned_word_count
            line_reduction = raw_lines - cleaned_lines
            st.metric("ì •ë¦¬ë¨", f"{cleaned_word_count:,}ê°œ ë‹¨ì–´", f"-{word_reduction} ë‹¨ì–´, -{line_reduction} ì¤„")
        else:
            st.metric("ì •ë¦¬ë¨", "ë¹„í™œì„±í™”", "ì„¤ì •ì—ì„œ í™œì„±í™” ê°€ëŠ¥")
    
    with col3:
        efficiency = (len(cleaned_transcript) / len(raw_transcript) * 100) if raw_transcript else 0
        st.metric("ì••ì¶•ë¥ ", f"{efficiency:.1f}%", "")

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤
    st.subheader("ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
    download_col1, download_col2 = st.columns([1, 1])
    
    with download_col1:
        st.download_button(
            "ğŸ“„ ì •ë¦¬ëœ ìë§‰ ë‹¤ìš´ë¡œë“œ (TXT)",
            data=cleaned_transcript.encode("utf-8"),
            file_name=f"transcript_cleaned_{vid}.txt",
            mime="text/plain",
        )
    
    with download_col2:
        if show_original:
            st.download_button(
                "ğŸ“„ ì›ë³¸ ìë§‰ ë‹¤ìš´ë¡œë“œ (TXT)",
                data=raw_transcript.encode("utf-8"),
                file_name=f"transcript_original_{vid}.txt",
                mime="text/plain",
            )

    # ìë§‰ ë‚´ìš© í‘œì‹œ
    st.subheader("ğŸ“œ ìë§‰ ë‚´ìš©")
    
    if show_original and cleaned_transcript != raw_transcript:
        # ì›ë³¸ê³¼ ì •ë¦¬ëœ ê²ƒì„ íƒ­ìœ¼ë¡œ ë¶„ë¦¬
        tab1, tab2 = st.tabs(["ğŸ§¹ ì •ë¦¬ëœ ìë§‰", "ğŸ“‹ ì›ë³¸ ìë§‰"])
        
        with tab1:
            st.text_area(
                "", 
                value=cleaned_transcript, 
                height=500,
                help="ì¤‘ë³µ ì œê±° ë° ë³‘í•©ì´ ì ìš©ëœ ìë§‰ì…ë‹ˆë‹¤",
                key="cleaned_transcript"
            )
        
        with tab2:
            st.text_area(
                "", 
                value=raw_transcript, 
                height=500,
                help="ì›ë³¸ ìë§‰ ê·¸ëŒ€ë¡œì…ë‹ˆë‹¤",
                key="original_transcript"
            )
    else:
        # í•˜ë‚˜ë§Œ í‘œì‹œ
        display_transcript = cleaned_transcript if (clean_duplicates or merge_consecutive) else raw_transcript
        st.text_area(
            "", 
            value=display_transcript, 
            height=500,
            help="ìë§‰ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ë³µì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
        )

# í•˜ë‹¨ ì •ë³´ ë° íŒ
st.markdown("---")
st.markdown("### ğŸ’¡ ì‚¬ìš© íŒ")

tip_col1, tip_col2 = st.columns([1, 1])

with tip_col1:
    st.markdown("""
    **ì°¨ë‹¨ ìš°íšŒ íŒ**:
    - ì—°ì† ì¶”ì¶œ ì‹œ 10ë¶„ ì´ìƒ ê°„ê²© ë‘ê¸°
    - VPN ì‚¬ìš©ìœ¼ë¡œ IP ë³€ê²½
    - ì‹œê°„ëŒ€ë³„ ì œí•œì´ ë‹¤ë¥´ë‹ˆ ë‹¤ë¥¸ ì‹œê°„ì— ì‹œë„
    - ë„ˆë¬´ ë§ì€ ì˜ìƒì„ í•œë²ˆì— ì²˜ë¦¬í•˜ì§€ ë§ê¸°
    """)

with tip_col2:
    st.markdown("""
    **ì¼ë°˜ ì‚¬ìš© íŒ**:
    - ê°œì¸ í•™ìŠµ/ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©
    - ì €ì‘ê¶Œ ë³´í˜¸ëœ ì½˜í…ì¸  ì£¼ì˜
    - ê¸´ ì˜ìƒì¼ìˆ˜ë¡ ì¶”ì¶œ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼
    - ìë§‰ ì •ë¦¬ ì˜µì…˜ìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ
    """)

# íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ
with st.expander("ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ"):
    st.markdown("""
    **ë¬¸ì œë³„ í•´ê²°ì±…**:
    
    1. **429 ì˜¤ë¥˜ (Too Many Requests)**
       - 10-30ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
       - VPNìœ¼ë¡œ IP ë³€ê²½
       - ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ ì‚¬ìš©
    
    2. **403 ì˜¤ë¥˜ (Forbidden)**
       - VPN ì‚¬ìš© í•„ìˆ˜
       - ë‹¤ë¥¸ êµ­ê°€ ì„œë²„ ì„ íƒ
       - ëª¨ë°”ì¼ ë„¤íŠ¸ì›Œí¬ ì‹œë„
    
    3. **ìë§‰ ì—†ìŒ ì˜¤ë¥˜**
       - YouTubeì—ì„œ ì§ì ‘ ìë§‰ í™•ì¸
       - ë‹¤ë¥¸ ì–¸ì–´ ìë§‰ ì‹œë„
       - ìë™ìƒì„± ìë§‰ í™œì„±í™” í™•ì¸
    
    4. **ì˜ìƒ ì ‘ê·¼ ë¶ˆê°€**
       - ì˜ìƒ ê³µê°œ ìƒíƒœ í™•ì¸
       - ì—°ë ¹/ì§€ì—­ ì œí•œ í™•ì¸
       - ì§ì ‘ YouTubeì—ì„œ ì‹œì²­ ê°€ëŠ¥í•œì§€ í™•ì¸
    
    5. **ì¼ë°˜ì ì¸ ì°¨ë‹¨ í˜„ìƒ**
       - í•˜ë£¨ì— 5-10ê°œ ì˜ìƒ ì´í•˜ë¡œ ì œí•œ
       - ê° ì¶”ì¶œ ê°„ ìµœì†Œ 2-3ë¶„ ê°„ê²©
       - í”„ë¡ì‹œë‚˜ VPN ìˆœí™˜ ì‚¬ìš©
    """)

st.caption("âš ï¸ ì´ ë„êµ¬ëŠ” êµìœ¡ ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”. YouTube ì„œë¹„ìŠ¤ ì•½ê´€ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”.")
